{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Version 3.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP HW 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing all the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in c:\\users\\palak\\anaconda3\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\palak\\anaconda3\\lib\\site-packages (from bs4) (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\palak\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.3.1)\n",
      "Requirement already satisfied: gensim in c:\\users\\palak\\anaconda3\\lib\\site-packages (4.1.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\palak\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\palak\\anaconda3\\lib\\site-packages (from gensim) (1.21.5)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\palak\\anaconda3\\lib\\site-packages (from gensim) (1.9.1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "# ! pip install bs4\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import word_tokenize\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC as SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from numpy import argmax\n",
    "from copy import deepcopy\n",
    "from numpy import vstack\n",
    "\n",
    "# ! pip install gensim\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "\n",
    "import gensim.models\n",
    "from gensim import utils\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Generation\n",
    "We will use the Amazon reviews dataset used in HW1. Load the dataset and\n",
    "build a balanced dataset of 60K reviews along with their ratings to create\n",
    "labels through random selection similar to HW1. You can store your dataset\n",
    "after generation and reuse it to reduce the computational load. For your\n",
    "experiments consider a 80%/20% training/testing split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the dataset in the cell below, by first reading the tsv file and then removing any row of data having inconsistent value, I also drop all the columns of the dataset except the review_body and the star_rating, since these are the only coulmns of use to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2rZ1rXUeXw4d",
    "outputId": "6a8d78ba-9c29-45b8-e4fa-dfc00dcd7ea6"
   },
   "outputs": [],
   "source": [
    "# copied_path = 'amazon_reviews_us_Beauty_v1_00.tsv.gz' \n",
    "copied_path = 'data.tsv' \n",
    "data = pd.read_table(copied_path, on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting the dataset classifications of 1,2, to 1; 3 to 2; and 4 and 5 to 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5NdN9QUzZpwC",
    "outputId": "40137bff-5800-4943-8c41-9af80ed16a88"
   },
   "outputs": [],
   "source": [
    "data['review_body']=data['review_body'].apply(str) \n",
    "dff = data.loc[:, ['star_rating','review_body']]\n",
    "df1=dff.loc[data['star_rating'].isin(['1','2'])]\n",
    "df1['star_rating']=1\n",
    "df2=dff.loc[data['star_rating'] == '3']\n",
    "df2['star_rating']=2\n",
    "df3=dff.loc[data['star_rating'].isin(['4','5'])]\n",
    "df3['star_rating']=3\n",
    "df1=df1.sample(n=20000)\n",
    "df2=df2.sample(n=20000)\n",
    "df3=df3.sample(n=20000)\n",
    "dfr=pd.concat([df1,df2,df3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "tJFOevSmfIZq",
    "outputId": "267855ad-2bcf-4bd5-82ce-972024909543"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1786360</th>\n",
       "      <td>1</td>\n",
       "      <td>The vial arrived empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286464</th>\n",
       "      <td>1</td>\n",
       "      <td>I hate giving bad reviews.  However, this scal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994118</th>\n",
       "      <td>1</td>\n",
       "      <td>This foundation looked absolutely beautiful wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306992</th>\n",
       "      <td>1</td>\n",
       "      <td>The one I received was not the one in the phot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280190</th>\n",
       "      <td>1</td>\n",
       "      <td>Didn't get the dvd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1533353</th>\n",
       "      <td>1</td>\n",
       "      <td>I was so excited about this bag.  It arrived q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1138979</th>\n",
       "      <td>1</td>\n",
       "      <td>Glue was already dry!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1558908</th>\n",
       "      <td>1</td>\n",
       "      <td>GREAT! My old brush attachment was getting pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159993</th>\n",
       "      <td>1</td>\n",
       "      <td>wasn't the same</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996361</th>\n",
       "      <td>1</td>\n",
       "      <td>I'm disappointed in Sonicare.  The wand units ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         star_rating                                        review_body\n",
       "1786360            1                             The vial arrived empty\n",
       "286464             1  I hate giving bad reviews.  However, this scal...\n",
       "994118             1  This foundation looked absolutely beautiful wh...\n",
       "306992             1  The one I received was not the one in the phot...\n",
       "280190             1                                 Didn't get the dvd\n",
       "1533353            1  I was so excited about this bag.  It arrived q...\n",
       "1138979            1                              Glue was already dry!\n",
       "1558908            1  GREAT! My old brush attachment was getting pre...\n",
       "159993             1                                    wasn't the same\n",
       "1996361            1  I'm disappointed in Sonicare.  The wand units ..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfr.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing review body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F0MFlW_6hyU1",
    "outputId": "b8df04a4-a71c-41a2-fbe3-0355dc72d439"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "dfr['review_body'] = dfr['review_body'].str.lower()\n",
    "dfr['review_body']=dfr['review_body'].apply(str)\n",
    " # strip html with BeautifulSoup\n",
    "dfr['review_body'] = [BeautifulSoup(text).get_text() for text in dfr['review_body'] ]\n",
    "# remove non alphabetic. keep spaces\n",
    "dfr['review_body'] = dfr['review_body'].str.replace('[^a-zA-Z ]', '')\n",
    "# strip leading and trailing spaces. strip extra white spaces\n",
    "dfr['review_body'] = dfr['review_body'].str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v58NdRd0bVCb"
   },
   "source": [
    "# TASK 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Loading the word2vec-google-news-300 pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PDyID0MiaRjL",
    "outputId": "aa15e66d-850d-4ae6-ce89-1bcdd7687a63"
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "wordtovec= api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Dxu4MFLLi71-"
   },
   "outputs": [],
   "source": [
    "# wordtovec.save('NLP_HW3/wordtovec.kv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "0n8nxp69iZ5L"
   },
   "outputs": [],
   "source": [
    "# from gensim.models import KeyedVectors\n",
    "# wordtovec = KeyedVectors.load('NLP_HW3/wordtovec.kv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the cosine similarity between 2 words on the pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below we check semantic similarities of the pretrained model on example:\n",
    "\n",
    "1) man ~ boy\n",
    "\n",
    "2) man ~ women\n",
    "\n",
    "3) white ~ color\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('good', 0.7190051078796387),\n",
       " ('terrible', 0.6828611493110657),\n",
       " ('horrible', 0.6702598333358765),\n",
       " ('Bad', 0.669891893863678),\n",
       " ('lousy', 0.6647640466690063),\n",
       " ('crummy', 0.567781925201416),\n",
       " ('horrid', 0.5651682615280151),\n",
       " ('awful', 0.5527253150939941),\n",
       " ('dreadful', 0.5526429414749146),\n",
       " ('horrendous', 0.5445998311042786)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordtovec.most_similar('bad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w0NKQptSjy-_",
    "outputId": "7bfda7cc-a727-495f-81ca-cd90d7accded"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.682487"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordtovec.similarity(\"man\", \"boy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yEbFPjTclY_G",
    "outputId": "fcb37ee3-8354-47a9-d0a5-dcb6a441dd79"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28830528"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordtovec.similarity(\"man\", \"women\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t0_JZC3dpNeC",
    "outputId": "68ff7725-171f-47b2-f627-fa92ec84b0d3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.51210797"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordtovec.similarity(\"white\", \"color\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Pre-trained Model: Father - Boy + Mother = husband, with a cosine similarity of 0.7670101523399353\n"
     ]
    }
   ],
   "source": [
    "print(\"For Pre-trained Model: Father - Boy + Mother = {}, with a cosine similarity of {}\".format(wordtovec.most_similar(positive=[\"father\", \"mother\"], negative=[\"boy\"], topn=1)[0][0],wordtovec.most_similar(positive=[\"father\", \"mother\"], negative=[\"boy\"], topn=1)[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Generating our own enerate Word2Vec features for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "HirRWykGbZGK"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Load and tokenize data\n",
    "\n",
    "tokenized_data = [row.split() for row in dfr.review_body]\n",
    "\n",
    "# Train Word2Vec model\n",
    "\n",
    "word_vec_model = Word2Vec(tokenized_data, #Word list\n",
    "                          vector_size=300, #Embedding size\n",
    "                          window=13, #Maximum Distance between current and predicted word\n",
    "                          min_count=9, #Ignore all words with total frequency lower than this \n",
    "                          workers=4  #Number of CPU Cores\n",
    "                         )\n",
    "\n",
    "# Save model\n",
    "# word_vec_model.save(\"NLP_HW3/word_vec_model.kv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ru-mk3vyqu5O"
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "word_vec_model = KeyedVectors.load('NLP_HW3/word_vec_model.kv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('strong', 0.5687657594680786),\n",
       " ('good', 0.5618583559989929),\n",
       " ('terrible', 0.5501324534416199),\n",
       " ('horrible', 0.5137025713920593),\n",
       " ('weird', 0.45193347334861755),\n",
       " ('overwhelming', 0.45024770498275757),\n",
       " ('nasty', 0.44143712520599365),\n",
       " ('fake', 0.43191030621528625),\n",
       " ('awful', 0.4203457534313202),\n",
       " ('unpleasant', 0.41711878776550293)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vec_model.wv.most_similar('bad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding cosine similairity between word on our word2vec model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below we check semantic similarities of the pretrained model on example:\n",
    "\n",
    "1) man ~ boy\n",
    "\n",
    "2) man ~ women\n",
    "\n",
    "3) white ~ color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "my1nSyeUrvMG",
    "outputId": "0938053f-41d8-4047-8e69-b8c369ea31a1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25545132"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vec_model.wv.similarity(\"man\", \"boy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YaKP_5VTrnIM",
    "outputId": "181a4b5d-3012-41a0-f481-f93c3c587bd2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49298048"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vec_model.wv.similarity(\"man\", \"women\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DQuY4eOrpmyH",
    "outputId": "a7bff3af-8b7e-44bd-8028-636c2ba65b37"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5235777"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vec_model.wv.similarity(\"white\", \"color\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Self-trained Model: Father - Boy + Mother = husband, with a cosine similarity of 0.6589816212654114\n"
     ]
    }
   ],
   "source": [
    "print(\"For Self-trained Model: Father - Boy + Mother = {}, with a cosine similarity of {}\".format(word_vec_model.wv.most_similar(positive=[\"father\", \"mother\"], negative=[\"boy\"], topn=1)[0][0],word_vec_model.wv.most_similar(positive=[\"father\", \"mother\"], negative=[\"boy\"], topn=1)[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What do you conclude from comparing vectors generated by yourself and the pretrained model?\n",
    "The pretrained model give the similarity for man and boy as 0.682487 and my model gives it as 0.25545132.\n",
    "The pretrained model give the similarity for man and woman as 0.28830528 and my model gives it as 0.49298048.\n",
    "The pretrained model give the similarity for white and color as 0.51210797 and my model gives it as 0.5235777.\n",
    "\n",
    "It states that the pretrain model is more accurate than the model trained by me as the similaity scores in higher for the similar words in pretrain model and lower for dissimilar words than my trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which of the Word2Vec models seems to encode semantic similarities between words better?\n",
    "The word2vec-google-news-300 Word2Vec model encode the semantic similarities between words better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aIeJTfz23YiV"
   },
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bq0_19cUGenw"
   },
   "source": [
    "### Training simple models using Google pre-trained Word2Vec features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "aE1gPs9QShue"
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "wordtovec = KeyedVectors.load('NLP_HW3/wordtovec.kv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "CuHyq8MPExZS"
   },
   "outputs": [],
   "source": [
    "tokenized_data = [row.split() for row in dfr.review_body]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##### Creating scentence vectors using each dataset wwords vector given by the Google pre-trained Word2Vec features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D_ZOYBVQfo4B",
    "outputId": "997e3de2-910e-4417-a05e-1e70d884de41"
   },
   "outputs": [],
   "source": [
    "#Convert Review to a Word List\n",
    "\n",
    "w2vvectors=[]                            #List to hold all word vectors in a lists of lists format\n",
    "for i in range(dfr.shape[0]):\n",
    "    rowvec=[]                             #List to hold all word vectors in each row\n",
    "    for word in tokenized_data[i]:\n",
    "        if(word in wordtovec):\n",
    "            rowvec.append(wordtovec.get_vector(word))\n",
    "        else:\n",
    "            rowvec.append(np.zeros(300, dtype=float))     #if word is not in the google word2vec model\n",
    "    w2vvectors.append(rowvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "RMxPC_I_n6Xq"
   },
   "outputs": [],
   "source": [
    "avgw2v=[]                             #list to hold the final vectors for each row\n",
    "for row in w2vvectors:\n",
    "    if(len(row)>0):\n",
    "        vecavg=sum(row)/len(row)       # to calculate the average of all the vectors in each scentence\n",
    "    else:\n",
    "        vecavg=[0]*300\n",
    "    avgw2v.append(vecavg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(avgw2v[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating the test train split to train the perceptron and SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "ayu_LVKir6T3"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(avgw2v, dfr['star_rating'], test_size = 0.2, random_state = 42)\n",
    "y_train = y_train.astype('int')\n",
    "y_test = y_test.astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R-BPpMJqCLsI"
   },
   "source": [
    "### Perceptron( word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c-bL77JoLv74",
    "outputId": "1aff744f-fe6b-4f60-e16e-3161eac24cf8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.74      0.54      0.63      5365\n",
      "           2       0.63      0.45      0.53      5584\n",
      "           3       0.24      0.90      0.37      1051\n",
      "\n",
      "    accuracy                           0.53     12000\n",
      "   macro avg       0.54      0.63      0.51     12000\n",
      "weighted avg       0.64      0.53      0.56     12000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import classification_report\n",
    "p = Perceptron(n_jobs = -1, max_iter = 10000, random_state = 42)\n",
    "p.fit(X_train, y_train)\n",
    "print(classification_report(p.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UqcRw8XdCPu1"
   },
   "source": [
    "### SVM(word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "_dk7s1aOCdwa",
    "outputId": "44703cd3-54bc-4175-8d45-3868fb31218e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.70      0.65      0.67      4266\n",
      "           2       0.57      0.60      0.59      3828\n",
      "           3       0.72      0.74      0.73      3906\n",
      "\n",
      "    accuracy                           0.67     12000\n",
      "   macro avg       0.67      0.67      0.66     12000\n",
      "weighted avg       0.67      0.67      0.67     12000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "# LinearSVC(max_iter=1000, random_state=42)\n",
    "lsvc = LinearSVC(max_iter=1000, random_state=42)\n",
    "lsvc.fit(X_train, y_train)\n",
    "print(classification_report(lsvc.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vx4iHSppGo0Z"
   },
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "A1TBVUCFGKYa"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "v = TfidfVectorizer()\n",
    "x = v.fit_transform(dfr['review_body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "_4l0TNkUGx68"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x, dfr['star_rating'], test_size = 0.2, random_state = 42)\n",
    "y_train = y_train.astype('int')\n",
    "y_test = y_test.astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gC_AoltHHMi1"
   },
   "source": [
    "### Perceptron(TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bCTzfn_SG0j6",
    "outputId": "fb9e5edc-fee2-4f18-f028-524a33e3d311"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.63      0.65      0.64      3837\n",
      "           2       0.51      0.55      0.53      3695\n",
      "           3       0.77      0.69      0.73      4468\n",
      "\n",
      "    accuracy                           0.64     12000\n",
      "   macro avg       0.64      0.63      0.63     12000\n",
      "weighted avg       0.64      0.64      0.64     12000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p = Perceptron(n_jobs = -1, max_iter = 10000, random_state = 42)\n",
    "p.fit(X_train, y_train)\n",
    "print(classification_report(p.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qefskKFpHPzx"
   },
   "source": [
    "### SVM(TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-DO7E9A0G1Sy",
    "outputId": "40f1dfe3-01bd-4d20-b887-a89c1ef3c4cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.72      0.70      0.71      4080\n",
      "           2       0.59      0.62      0.61      3820\n",
      "           3       0.80      0.78      0.79      4100\n",
      "\n",
      "    accuracy                           0.70     12000\n",
      "   macro avg       0.70      0.70      0.70     12000\n",
      "weighted avg       0.71      0.70      0.70     12000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "# LinearSVC(max_iter=1000, random_state=42)\n",
    "lsvc = LinearSVC(max_iter=1000, random_state=42)\n",
    "lsvc.fit(X_train, y_train)\n",
    "print(classification_report(lsvc.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do you conclude from comparing performances for the models trained using the two different feature types (TF-IDF and your trained Word2Vec features)?\n",
    "\n",
    "Tf-idf vectorization outstands the performance of Word2Vec in Simple models like Perceptron and SVM. As accuracy for Perceptron is 0.64 and 0.53 for tf-idf and word2vec respectively and accuracy for SVM is 0.70 and 0.67 for tf-idf and word2vec respectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tDdEgolKHX2z"
   },
   "source": [
    "# TASK 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(avgw2v, dfr['star_rating'], test_size = 0.2, random_state = 42)\n",
    "y_train = y_train.astype('int')\n",
    "y_test = y_test.astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Converting our train and test splits to np array and then tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.int64)\n",
    "y_test = torch.tensor(y_test, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Defining a class for Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    # Initialize the MLP's layers and activation functions\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, num_classes):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)    # First fully connected layer with input_size input features and hidden_size1 output features\n",
    "        self.relu1 = nn.ReLU()                            # ReLU activation function applied to the output of fc1\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)   # Second fully connected layer with hidden_size1 input features and hidden_size2 output features\n",
    "        self.relu2 = nn.ReLU()                            # ReLU activation function applied to the output of fc2\n",
    "        self.fc3 = nn.Linear(hidden_size2, num_classes)    # Third fully connected layer with hidden_size2 input features and num_classes output features\n",
    "\n",
    "    # Define the forward pass through the MLP's layers\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)           # Pass the input x through the first fully connected layer\n",
    "        out = self.relu1(out)       # Apply ReLU activation function to the output of fc1\n",
    "        out = self.fc2(out)         # Pass the output of ReLU through the second fully connected layer\n",
    "        out = self.relu2(out)       # Apply ReLU activation function to the output of fc2\n",
    "        out = self.fc3(out)         # Pass the output of ReLU through the third fully connected layer\n",
    "        return out                  # Return the final output of the MLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define hyperparameters \n",
    "input_size = 300            #As this the size our vector\n",
    "hidden_size1 = 100\n",
    "hidden_size2 = 10\n",
    "output_size = 4            #As we want to classify it into 3 classes; 4 because we cant to classify it into 1,2 and 3 but classes are 0,1,2,3\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "# Initialize the model, criterion and optimizer\n",
    "model = MLP(input_size, hidden_size1, hidden_size2, output_size)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Loop over batches of training data\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        # Zero out the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Compute the forward pass\n",
    "        outputs = model(X_train[i:i+batch_size])\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, y_train[i:i+batch_size])\n",
    "        \n",
    "        # Compute the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing the model and finiding out the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.6044\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(X_test)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    accuracy = (predicted == y_test).sum().item() / y_test.size(0)\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To generate the input features, concatenate the first 10 Word2Vec vectors for each review as the input feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "first10w2v=[]                             #list for final vectors for each scentence having length 3000 after concating 10 words vectors\n",
    "for i in range(dfr.shape[0]):\n",
    "    rowvec=[]                             #list to iterate over the words in each row\n",
    "    for j in range(10):\n",
    "        if(j<len(tokenized_data[i])):     #if the scentence is greater than 10\n",
    "            if(tokenized_data[i][j] in wordtovec):\n",
    "                rowvec.extend(wordtovec.get_vector(tokenized_data[i][j]))\n",
    "            else:\n",
    "                rowvec.extend(np.zeros(300, dtype=float))\n",
    "        else:                             # else padding is done to make it size of 10\n",
    "            rowvec.extend(np.zeros(300, dtype=float))\n",
    "    first10w2v.append(rowvec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Spliting into train and test and then converting them into tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(first10w2v, dfr['star_rating'], test_size = 0.2, random_state = 42)\n",
    "y_train = y_train.astype('int')\n",
    "y_test = y_test.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.int64)\n",
    "y_test = torch.tensor(y_test, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 3000           #As the concatinated vector is of size 3000 i.e. 300*10 words\n",
    "hidden_size1 = 100\n",
    "hidden_size2 = 10\n",
    "output_size = 4\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(input_size, hidden_size1, hidden_size2, output_size)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train the model on train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    # Loop over batches of training data\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        # Zero out the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Compute the forward pass\n",
    "        outputs = model(X_train[i:i+batch_size])\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, y_train[i:i+batch_size])\n",
    "        \n",
    "        # Compute the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the parameters\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test the model and find out the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.5769\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(X_test)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    accuracy = (predicted == y_test).sum().item() / y_test.size(0)\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do you conclude by comparing accuracy values you obtain with those obtained in the “’Simple Models” section.\n",
    "\n",
    "The accuracy of Simple Models are 0.53 and 0.67 for perceptron and svm respectively. Here, the MLP gives accuracy 0.6044 and 0.5769 with simple model(SVM) performing better than FNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating word2vec vectors with the maximum limit of 20 words and padding the shorter reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = [row.split() for row in dfr.review_body]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "first20w2v=[]\n",
    "for i in range(dfr.shape[0]):\n",
    "    rowvec=[]\n",
    "    for j in range(20):\n",
    "        if(j<len(tokenized_data[i])):\n",
    "            if(tokenized_data[i][j] in wordtovec):\n",
    "                rowvec.append(wordtovec.get_vector(tokenized_data[i][j]))\n",
    "            else:\n",
    "                rowvec.append(np.zeros(300, dtype=float))\n",
    "        else:\n",
    "            rowvec.append(np.zeros(300, dtype=float))\n",
    "    first20w2v.append(rowvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgw2v=[]\n",
    "for row in first20w2v:\n",
    "    if(len(row)>0):\n",
    "        vecavg=sum(row)/len(row)\n",
    "    else:\n",
    "        vecavg=[0]*300\n",
    "    avgw2v.append(vecavg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### spliting into train and test and converting them into tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(avgw2v, dfr['star_rating'], test_size = 0.2, random_state = 42)\n",
    "y_train = y_train.astype('int')\n",
    "y_test = y_test.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.int64)\n",
    "y_test = torch.tensor(y_test, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  RNN Model with hidden state size of 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch\n",
    "class RNN_Data(Dataset):    \n",
    "    # Constructor method, initializes X_data and Y_data\n",
    "    def __init__(self, X_data, Y_data):\n",
    "        self.X_data = X_data\n",
    "        self.Y_data = Y_data\n",
    "\n",
    "    # Length method, returns the length of the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "    # Getitem method, returns a single item from the dataset at the given index\n",
    "    def __getitem__(self, index):\n",
    "        # Create a zero-filled numpy array of size (20, 300) as padding\n",
    "        pad = np.zeros((20, 300), dtype=float)\n",
    "\n",
    "        # Fill the last rows of the padding array with the current X_data element\n",
    "        pad[-len(self.X_data[index]):] = np.array(self.X_data[index])\n",
    "\n",
    "        # Convert the padded array to a PyTorch FloatTensor object\n",
    "        X = torch.FloatTensor(pad)\n",
    "\n",
    "        # Convert the current Y_data element to a PyTorch tensor object\n",
    "        Y = torch.tensor(self.Y_data[index])\n",
    "\n",
    "        # Return the X and Y tensors as a tuple\n",
    "        return X, Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_collate(batch):\n",
    "    data = [item[0] for item in batch]\n",
    "    target = [item[1] for item in batch]\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the RNN Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.214972\n",
      "Epoch: 2 \tTraining Loss: 0.580988\n",
      "Epoch: 3 \tTraining Loss: 0.886190\n",
      "Epoch: 4 \tTraining Loss: 0.895759\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "# Define training parameters\n",
    "input_size = 300\n",
    "output_size = 4\n",
    "hidden_dim = 20\n",
    "n_layers = 1\n",
    "\n",
    "#Define a RNN layer with input_size, hidden_dim, n_layers and batch_first=True\n",
    "layer = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)\n",
    "# Define a fully connected linear layer with input size of 400 and output size of output_size\n",
    "fc = nn.Linear(400, output_size)                                        \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# Define a function to initialize the hidden state of the RNN layer with zeros\n",
    "def init_hidden(batch_size):\n",
    "    hidden = torch.zeros(n_layers, batch_size, hidden_dim)\n",
    "    return hidden\n",
    "\n",
    "\n",
    "# Define a function to perform forward pass through the RNN and linear layers\n",
    "def rnn_model(x):\n",
    "    batch_size = x.size(0)\n",
    "    hidden = init_hidden(batch_size)\n",
    "    out, hidden = layer(x, hidden)\n",
    "    out = out.contiguous().view(-1, out.shape[1] * out.shape[2])\n",
    "    out = fc(out)\n",
    "    return out, hidden\n",
    "\n",
    "\n",
    "# Create a custom dataset object \"rnn_train\" using the \"RNN_Data\" class and pass training data\n",
    "rnn_train = RNN_Data(X_train,y_train)\n",
    "train_loader_mode = DataLoader(dataset=rnn_train, batch_size=8, shuffle=True, collate_fn=my_collate, drop_last=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion.to(device)\n",
    "optimizer = torch.optim.Adam(list(layer.parameters()) + list(fc.parameters()), lr=0.0001)\n",
    "\n",
    "for ep in range(1, 5):\n",
    "    \n",
    "    # Loop through each batch in the training data\n",
    "    for input_data, label in train_loader_mode:\n",
    "\n",
    "        # Zero out the gradients in the optimizer\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Stack the input data and labels as PyTorch tensors and move to the device\n",
    "        input_data = torch.stack(input_data)\n",
    "        label = torch.stack(label)\n",
    "        input_data = input_data.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        # Perform forward pass through the RNN and linear layers\n",
    "        output, hidden = rnn_model(input_data)\n",
    "\n",
    "        # Calculate the loss using the CrossEntropyLoss function and the predicted output and true labels\n",
    "        loss = criterion(output, label)\n",
    "\n",
    "        # Perform backpropagation to calculate gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the weights in the optimizer\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print the epoch number and training loss\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(ep, loss.item()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the  RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN has an Accuracy: 0.6374166666666666\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "rnn_test = RNN_Data(X_test, y_test)\n",
    "test_loader_mode = DataLoader(dataset = rnn_test, batch_size=8, collate_fn=my_collate, drop_last=True)\n",
    "\n",
    "predictions, actual = list(), list()             #Initialize empty lists to store predicted labels and actual labels.\n",
    "for test_data, test_label in test_loader_mode:\n",
    "    test_data = torch.stack(test_data)\n",
    "    test_label = torch.stack(test_label)\n",
    "    pred, hid = rnn_model(test_data.to('cpu'))    #Pass the test data through the RNN model to obtain the predicted output and hidden state.\n",
    "    pred = pred.to('cpu')\n",
    "    pred = pred.detach().numpy()                 # Detach the predicted output from the computation graph and convert it to a NumPy array\n",
    "    pred = argmax(pred, axis= 1)                 #Get the index of the maximum value in the predicted output along the second axis.\n",
    "    target = test_label.numpy()\n",
    "    target = target.reshape((len(target), 1))\n",
    "    pred = pred.reshape((len(pred)), 1)\n",
    "    pred = pred.round()\n",
    "    predictions.append(pred)\n",
    "    actual.append(target)\n",
    "\n",
    "predictions, actual = vstack(predictions), vstack(actual)         #vertically stack the predicted and actual labels into a single array.\n",
    "acc = accuracy_score(actual, predictions)\n",
    "print('RNN has an Accuracy: {}'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do you conclude by comparing accuracy values you obtain with those obtained with feedforward neural network models.\n",
    "The accuracy for RNN model considering sentence vectors for first 20 words and padding the shorter sentences is 0.6374166666666666. The FNN model considering sentence vectors of whole sentence is 0.6044 and while considering combined vectors of first 10 words is 0.5769. So, the accuracy is better with RNN model than FNN model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the GRU Model\n",
    "##### Trained in a similar way as the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.057024\n",
      "Epoch: 2 \tTraining Loss: 0.512950\n",
      "Epoch: 3 \tTraining Loss: 0.748324\n",
      "Epoch: 4 \tTraining Loss: 0.776006\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "input_size = 300\n",
    "output_size = 4\n",
    "hidden_dim = 20\n",
    "n_layers = 1\n",
    "\n",
    "layer = nn.GRU(input_size, hidden_dim, n_layers, batch_first=True)\n",
    "fc = nn.Linear(400, output_size)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def init_hidden(batch_size):\n",
    "    hidden = torch.zeros(n_layers, batch_size, hidden_dim)\n",
    "    return hidden\n",
    "\n",
    "def gru_model(x):\n",
    "    batch_size = x.size(0)\n",
    "    hidden = init_hidden(batch_size)\n",
    "    out, hidden = layer(x, hidden)\n",
    "    out = out.contiguous().view(-1, out.shape[1] * out.shape[2])\n",
    "    out = fc(out)\n",
    "    return out, hidden\n",
    "\n",
    "rnn_train = RNN_Data(X_train,y_train)\n",
    "train_loader_mode = DataLoader(dataset = rnn_train,batch_size=8, shuffle = True, collate_fn=my_collate, drop_last=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion.to(device)\n",
    "optimizer = torch.optim.Adam(list(layer.parameters()) + list(fc.parameters()), lr=0.0001)\n",
    "\n",
    "for ep in range(1, 5):\n",
    "    for input_data, label in train_loader_mode:\n",
    "        optimizer.zero_grad()\n",
    "        input_data = torch.stack(input_data)\n",
    "        label = torch.stack(label)\n",
    "        output, hidden = gru_model(input_data.to(device))\n",
    "        label = label.to(device)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(ep, loss.item()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRU has an Accuracy: 0.63725\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "gru_test = RNN_Data(X_test, y_test)\n",
    "test_loader_mode = DataLoader(dataset = gru_test, batch_size=8, collate_fn=my_collate, drop_last=True)\n",
    "\n",
    "predictions, actual = list(), list()\n",
    "for test_data, test_label in test_loader_mode:\n",
    "    test_data = torch.stack(test_data)\n",
    "    test_label = torch.stack(test_label)\n",
    "    pred, hid = gru_model(test_data.to('cpu'))\n",
    "    pred = pred.to('cpu')\n",
    "    pred = pred.detach().numpy()\n",
    "    pred = argmax(pred, axis= 1)\n",
    "    target = test_label.numpy()\n",
    "    target = target.reshape((len(target), 1))\n",
    "    pred = pred.reshape((len(pred)), 1)\n",
    "    pred = pred.round()\n",
    "    predictions.append(pred)\n",
    "    actual.append(target)\n",
    "\n",
    "predictions, actual = vstack(predictions), vstack(actual)\n",
    "acc = accuracy_score(actual, predictions)\n",
    "print('GRU has an Accuracy: {}'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the LSTM Model\n",
    "##### Training it in a similar way as the RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.804450\n",
      "Epoch: 2 \tTraining Loss: 0.736467\n",
      "Epoch: 3 \tTraining Loss: 1.105232\n",
      "Epoch: 4 \tTraining Loss: 0.776341\n",
      "Epoch: 5 \tTraining Loss: 0.923788\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "input_size = 300\n",
    "output_size = 4\n",
    "hidden_dim = 20\n",
    "n_layers = 1\n",
    "\n",
    "layer = nn.LSTM(input_size, hidden_dim, n_layers, batch_first=True)\n",
    "fc = nn.Linear(400, output_size)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def init_hidden(batch_size):\n",
    "    # The LSTM model has 2 hidden layers\n",
    "    hidden = (torch.zeros(n_layers, batch_size, hidden_dim),torch.zeros(n_layers, batch_size, hidden_dim))\n",
    "    return hidden\n",
    "\n",
    "def lstm_model(x):\n",
    "    batch_size = x.size(0)\n",
    "    hidden = init_hidden(batch_size)\n",
    "    out, hidden = layer(x, hidden)\n",
    "    out = out.contiguous().view(-1, out.shape[1] * out.shape[2])\n",
    "    out = fc(out)\n",
    "    return out, hidden\n",
    "\n",
    "rnn_train = RNN_Data(X_train,y_train)\n",
    "train_loader_mode = DataLoader(dataset = rnn_train,batch_size=8, shuffle = True, collate_fn=my_collate, drop_last=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion.to(device)\n",
    "optimizer = torch.optim.Adam(list(layer.parameters()) + list(fc.parameters()), lr=0.0001)\n",
    "\n",
    "for ep in range(1, 6):\n",
    "    for input_data, label in train_loader_mode:\n",
    "        optimizer.zero_grad()\n",
    "        input_data = torch.stack(input_data)\n",
    "        label = torch.stack(label)\n",
    "        output, hidden = lstm_model(input_data.to(device))\n",
    "        label = label.to(device)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(ep, loss.item()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM has an Accuracy: 0.6286666666666667\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "lstm_test = RNN_Data(X_test, y_test)\n",
    "test_loader_mode = DataLoader(dataset = lstm_test, batch_size=8, collate_fn=my_collate, drop_last=True)\n",
    "\n",
    "predictions, actual = list(), list()\n",
    "for test_data, test_label in test_loader_mode:\n",
    "    test_data = torch.stack(test_data)\n",
    "    test_label = torch.stack(test_label)\n",
    "    pred, hid = lstm_model(test_data.to('cpu'))\n",
    "    pred = pred.to('cpu')\n",
    "    pred = pred.detach().numpy()\n",
    "    pred = argmax(pred, axis= 1)\n",
    "    target = test_label.numpy()\n",
    "    target = target.reshape((len(target), 1))\n",
    "    pred = pred.reshape((len(pred)), 1)\n",
    "    pred = pred.round()\n",
    "    predictions.append(pred)\n",
    "    actual.append(target)\n",
    "\n",
    "predictions, actual = vstack(predictions), vstack(actual)\n",
    "acc = accuracy_score(actual, predictions)\n",
    "print('LSTM has an Accuracy: {}'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do you conclude by comparing accuracy values you obtain by GRU, LSTM, and simple RNN.\n",
    "The accuracy are 0.6374166666666666, 0.63725, 0.6324166666666666 for simple RNN, GRU and LSTM respectively. Over all they give the similar accuracies. But, the accuracy for RNN is just slightly higher than the other two. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
